{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from Bio import AlignIO\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from src.sup_data_to_fasta import load_xlsx, transform_raw_dfs_to_queries\n",
    "\n",
    "sup5_file_path = Path(\"data/Liu_sup5_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória dos Dados Suplementares 5\n",
    "\n",
    "## Sobre\n",
    "\n",
    "Este notebook contém uma análise exploratória dos dados fornecidos por Liu, et al. No [suplemento 5](https://www.nature.com/articles/s41467-023-43632-1#additional-information\n",
    ") com o objetivo de averiguar os tratamentos executados no artigo\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "* Verificar a origem das tags dos pares sRNA/mRNA dos dados suplementares.\n",
    "* Ganhar mais insight sobre as interações medidas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_alignments():\n",
    "    output_dir = Path(\"output\")\n",
    "    alignments_files = output_dir.glob(\"*alignments_results.tsv\")\n",
    "    \n",
    "    exp_word_size_list = []\n",
    "\n",
    "    alignments_df_list = []\n",
    "    for file_path in alignments_files:\n",
    "        prefix = file_path.stem.split(\"_\")[0]\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        word_sz = prefix.split(\"w\")[1]\n",
    "        experiment = prefix.split(\"-\")[0]\n",
    "        exp_word_size_list.append((experiment, word_sz))\n",
    "        df[\"experiment\"] = experiment\n",
    "        df[\"word_sz\"] = word_sz\n",
    "        alignments_df_list.append(df)\n",
    "    \n",
    "    alignments_dict = {(exp, word_sz): df for (exp, word_sz), df in zip(exp_word_size_list, alignments_df_list)}\n",
    "    return alignments_dict, exp_word_size_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments_dict, exp_word_size_list = load_alignments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries():\n",
    "    sup5_file_path = Path(\"data/Liu_sup5_data.xlsx\")\n",
    "    dict = load_xlsx(sup5_file_path)\n",
    "    queries_dict = transform_raw_dfs_to_queries(dict)\n",
    "    queries_dict = {k.split('-')[0]:v for k,v in queries_dict.items()}\n",
    "    return queries_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_dict = load_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#queries 420\n",
      "#alignments 6485\n"
     ]
    }
   ],
   "source": [
    "queries_df = queries_dict['EP']\n",
    "alignments_df = alignments_dict[('EP', '11')]\n",
    "print(f\"#queries {len(queries_df)}\")\n",
    "print(f\"#alignments {len(alignments_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dfs(queries_df, alignments_df):\n",
    "    joined_df = pd.merge(queries_df, alignments_df, left_on='name', right_on='qseqid', how='left')\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_queries_dict = {}\n",
    "for (exp, word_sz) in exp_word_size_list:\n",
    "    queries_df = queries_dict[exp]\n",
    "    alignments_df = alignments_dict[(exp, word_sz)]\n",
    "    joined_df = join_dfs(queries_df, alignments_df)\n",
    "    aligned_queries_dict[(exp, word_sz)] = joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('SP', '11'), ('ESP', '10'), ('ESP', '9'), ('SP', '7'), ('ESP', '8'), ('ESP', '11'), ('SP', '10'), ('EP', '9'), ('EP', '8'), ('SP', '9'), ('EP', '11'), ('SP', '8'), ('ESP', '7'), ('EP', '10'), ('EP', '7')])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
